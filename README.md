# ResearchToolBench

A research agent benchmark combining concepts from the **AgentBeats Custom Tracks**:

## ğŸ† Challenge Compatibility

### Ï„Â²-Bench Challenge (Sierra Research)
- **Dual-control environments**: Both agent AND user have tools (Dec-POMDP)
- **Database state tracking**: Final state compared with expected state
- **pass^k reliability metric**: Measures consistency across trials
- **Three domains**: academic, news, technical (dual-control)

### OpenEnv Challenge (Meta-PyTorch / Hugging Face / Unsloth)
- **Gymnasium-style APIs**: `step()`, `reset()`, `state()`, `close()`
- **HuggingFace Hub compatible**: Ready for deployment
- **RL training ready**: TRL/TorchForge integration

## ğŸ“Š Evaluation Metrics

| Metric | Weight | Description |
|--------|--------|-------------|
| Tool Use | 20% | Required tools called correctly |
| Source Citation | 20% | Required sources cited |
| Fact Accuracy | 25% | Expected facts present |
| Policy Compliance | 15% | Domain policies followed |
| DB State (Ï„Â²-bench) | 20% | Database state matches expected |

**Additional Metrics:**
- `pass@1`: Single-trial success rate
- `pass@2`: Two-trial consistency (Ï„Â²-bench style)

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ResearchToolBench                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  Academic   â”‚    â”‚    News     â”‚    â”‚  Technical  â”‚ â”‚
â”‚  â”‚  (single)   â”‚    â”‚  (single)   â”‚    â”‚(DUAL-CTRL)  â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  OpenEnv APIs: step() | reset() | state() | close()    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Ï„Â²-bench: User Tools | DB State | pass^k Metric       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ Quick Start

### OpenEnv-Style Usage

```python
from src.green_agent import ResearchToolBenchEnv, RESEARCH_TASKS

# Create environment
env = ResearchToolBenchEnv()

# Reset with task
obs = env.reset(task_id="technical_pytorch_troubleshoot")

# Run episode
while not obs.done:
    action = your_agent.act(obs)  # Your purple agent
    result = env.step(action)
    obs = result.observation
    
    if result.done:
        print(f"Score: {result.info['evaluation']['total_score']}")

# Get final state (Ï„Â²-bench style)
state = env.state()
print(f"DB State: {state.database}")

env.close()
```

### Dual-Control Example (Ï„Â²-bench style)

```python
# In technical domain, USER has tools too!
obs = env.reset(task_id="technical_pytorch_troubleshoot")

print(f"Agent tools: {[t['name'] for t in obs.agent_tools]}")
print(f"User tools: {[t['name'] for t in obs.user_tools]}")  # Ï„Â²-bench!

# Agent requests user action
action = {
    "type": "tool_call",
    "tool_name": "request_user_action",
    "parameters": {"action": "pip install torch"}
}
result = env.step(action)

# User responds with tool usage
if "user_tool_call" in result.info:
    print(f"User used: {result.info['user_tool_call']['tool']}")
```

## ğŸ“ Project Structure

```
researchbench/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ green_agent.py    # Benchmark environment + evaluator
â”‚   â””â”€â”€ purple_agent.py   # Baseline agent
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â””â”€â”€ SUBMISSION_ANSWERS.md
```

## ğŸ³ Docker Deployment

```bash
# Build (IMPORTANT: use linux/amd64 for AgentBeats)
docker build --platform linux/amd64 -t researchbench-green .

# Run green agent
docker run -p 8000:8000 researchbench-green

# Run purple agent
docker run -p 8001:8001 researchbench-purple
```

## ğŸ“š References

- **Ï„Â²-bench**: Barres et al., 2025 ([arXiv:2506.07982](https://arxiv.org/abs/2506.07982))
- **Ï„-bench**: Yao et al., 2024 ([arXiv:2406.12045](https://arxiv.org/abs/2406.12045))
- **OpenEnv**: [github.com/meta-pytorch/OpenEnv](https://github.com/meta-pytorch/OpenEnv)
- **AgentBeats**: [rdi.berkeley.edu/agentx-agentbeats](https://rdi.berkeley.edu/agentx-agentbeats)

## ğŸ“„ License

MIT License
